{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.2 DataFrame to and from csv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvSl9LLkI8jFD5JNVyZGCX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edelord/DS-practice/blob/main/3_2_DataFrame_to_and_from_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMto43oUGCBu"
      },
      "source": [
        "https://www.gormanalysis.com/blog/python-pandas-for-your-grandpa-3-2-dataframe-to-and-from-csv/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjIt-SUoGByJ"
      },
      "source": [
        "In this section, we’ll see how you can read from and write to CSV files with Pandas.\n",
        "\n",
        "One of the most common ways to make a DataFrame is to load it from some pre-existing CSV file. Pandas has an awesome CSV reader for this, but before we use it, let’s make a DataFrame from scratch and write it to CSV so we have something to read. Here I’ll make a DataFrame called df with three columns and a thousand rows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6vz1c1tGp5L"
      },
      "source": [
        "https://numpy.org/doc/stable/reference/generated/numpy.arange.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbWJ_96CFwUK",
        "outputId": "ed1814fd-268f-4ea6-d7c6-213bc141a883"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'id': np.arange(1000),\n",
        "    'b': np.random.normal(size=1000),\n",
        "    'c': pd.Series(np.random.choice(['cat', 'dog', 'hippo'], size=1000, replace=True), dtype=\"string\")\n",
        "})\n",
        "print(df)\n",
        "##       id         b      c\n",
        "## 0      0  0.255534    dog\n",
        "## 1      1  0.454496  hippo\n",
        "## 2      2 -0.868720    dog\n",
        "## 3      3 -0.275906  hippo\n",
        "## 4      4  0.045178  hippo\n",
        "## ..   ...       ...    ...\n",
        "## 995  995 -0.376002    cat\n",
        "## 996  996 -0.645262    cat\n",
        "## 997  997  0.041527  hippo\n",
        "## 998  998  0.785299  hippo\n",
        "## 999  999 -0.183583    dog\n",
        "## \n",
        "## [1000 rows x 3 columns]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id         b      c\n",
            "0      0  1.033355  hippo\n",
            "1      1  0.507673    cat\n",
            "2      2  2.144893  hippo\n",
            "3      3  0.275436    cat\n",
            "4      4 -1.167548  hippo\n",
            "..   ...       ...    ...\n",
            "995  995 -0.043672    cat\n",
            "996  996 -0.478434    dog\n",
            "997  997  0.140751    cat\n",
            "998  998  1.003178    cat\n",
            "999  999 -1.038644    cat\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-Q21h3HGs7N"
      },
      "source": [
        "o write this DataFrame to CSV it’s pretty simple. You just use the built in DataFrame.to_csv() method and pass in a name for the file. For example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuL0yTX7G1yl"
      },
      "source": [
        "df.to_csv(path_or_buf='../df_export.csv')\n",
        "df.to_csv('df_export.csv', index=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z_c6f5aIK_S"
      },
      "source": [
        "By default, the file will get written to your current working directory and if you don’t know where that is, you can check it by importing os and running os.getcwd()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-G9jxRMGIMPw",
        "outputId": "b7aceb51-1a73-4251-b5b2-93206841fab9"
      },
      "source": [
        "import os\n",
        "\n",
        "os.getcwd()             # /your/current/working/directory :    '/content'"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRVj7cu-I7xH"
      },
      "source": [
        "Alternatively, you can specify the file path you want to write the data to using the path_or_buf parameter, like df.to_csv(path_or_buf = '/some/special/path/pets.csv').\n",
        "\n",
        "Also by default, to_csv() includes the row index in the output. So if you open the CSV file in a text editor don’t be surprised to find a nameless column at the front. For me, 9 times out of 10 I don’t want this, and it’s easy to prevent it by setting index = False.\n",
        "\n",
        "If you look at the documentation for to_csv(), you’ll find there are a ton of parameters that let you really customize the way a DataFrame is written to CSV. We won’t go through them here since it would get boring fast, but they’re pretty well documented so I’ll encourage you to peruse them on your own time.\n",
        "\n",
        "Now that we have a CSV file on disc, lets try loading it into a new DataFrame called pets. In this case, we’ll use the global function, pd.read_csv(), passing in the name of the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9rH0zE2JtLP",
        "outputId": "bdc9b4ea-96d2-4452-9735-30c146e3c60c"
      },
      "source": [
        "pets = pd.read_csv(\"df_export.csv\")\n",
        "print(pets)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      id         b      c\n",
            "0      0  1.033355  hippo\n",
            "1      1  0.507673    cat\n",
            "2      2  2.144893  hippo\n",
            "3      3  0.275436    cat\n",
            "4      4 -1.167548  hippo\n",
            "..   ...       ...    ...\n",
            "995  995 -0.043672    cat\n",
            "996  996 -0.478434    dog\n",
            "997  997  0.140751    cat\n",
            "998  998  1.003178    cat\n",
            "999  999 -1.038644    cat\n",
            "\n",
            "[1000 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWlzOF6XKcJZ"
      },
      "source": [
        "It might not seem that impressive, but if you’ve ever tried reading data from a file using a lower level language like C or C++, you should appreciate everything that’s happening under the hood. Not only did Pandas recognize the column names but it also interpreted the column data types correctly without any help from us.\n",
        "\n",
        "Of course, in the real world, things don’t always go so smoothly. Sometimes you’ll have to steer read_csv() in the right direction using some parameters like\n",
        "\n",
        "sep to specify a value separator if your file is something other than comma-delimited\n",
        "header to tell pandas if your file contains column names\n",
        "index_col to indicate which column if any should be used as the row index\n",
        "usecols to tell pandas “only read a certain subset of columns”\n",
        "Since read_csv() has even more parameters than to_csv(), we won’t cover all of them either, but they’re also well documented so, again, I’ll encourage you to read through them yourself."
      ]
    }
  ]
}